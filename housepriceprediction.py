# -*- coding: utf-8 -*-
"""HousePricePrediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wcowapNSlCCQAVkK33Cpk-xyFhevPAbp
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('train.csv')
df.head()

nan_feature = [features for features in df.columns if df[features].isnull().sum()>1]

for feature in nan_feature:
  print(feature, np.round(df[feature].isnull().mean(), 2), '% missing values')

for feature in nan_feature:
  df1 = df.copy()
  df1[feature] = np.where(df1[feature].isnull(), 1, 0)
  df1.groupby(feature)['SalePrice'].median().plot.bar()
  plt.title(feature)
  plt.show()

numer_features = [features for features in df.columns if df[features].dtype != 'O']
print('No. of num features: ', len(numer_features))

year_feature = [features for features in numer_features if 'Yr' in features or 'Year' in features]
year_feature

for features in year_feature:
  print(features, df[features].unique())

df.groupby('YrSold')['SalePrice'].median().plot()
plt.xlabel('Year Sold')
plt.ylabel('Price')
plt.title('YrSold vs SalePrice')
plt.show()

for features in year_feature:
  if features != 'YrSold':
    df1 = df.copy()
    df1[features] = df1['YrSold'] - df1[features]

    plt.scatter(df1[features], df1['SalePrice'], c = 'r', alpha= 0.2)
    plt.xlabel(features)
    plt.ylabel('SalePrice')
    plt.show()

discrete_feature = [features for features in numer_features if len(df[features].unique()) <25
                    and features not in year_feature+['Id']]
print('No. of discrete features: ', len(discrete_feature))

for features in discrete_feature:
  df1 = df.copy()
  df.groupby(features)['SalePrice'].median().plot.bar()
  plt.xlabel(features)
  plt.ylabel('SalePrice')
  plt.show()

continuous_features= [features for features in numer_features if features not in discrete_feature+
                     year_feature+['Id']]
print('No. of continuous features: ', len(continuous_features))

df[continuous_features].head()

for features in continuous_features:
  df1 = df.copy()
  df1[features].hist(bins= 20, color= 'y')
  plt.xlabel(features)
  plt.ylabel('SalePrice')
  plt.show()

for features in continuous_features:
  df1 = df.copy()
  if 0 in df1[features].unique():
    pass
  else:
    df1[features]= np.log(df1[features])
    df1['SalePrice']= np.log(df1['SalePrice'])
    plt.scatter(df1[features], df1['SalePrice'], c = 'r', alpha= 0.2)
    plt.xlabel(features)
    plt.ylabel('SalePrice')
    plt.show()

for features in continuous_features:
  df1= df.copy()
  if 0 in df1[features].unique():
    pass
  else:
    df1[features]= np.log(df1[features])
    df1.boxplot(column= features)
    plt.ylabel(features)
    plt.title(features)
    plt.show()

categorical_features = [features for features in df.columns if df[features].dtypes == 'O']
print('No. of categorical features: ', len(categorical_features))

for features in categorical_features:
  df1 = df.copy()
  df1.groupby(features)['SalePrice'].median().plot.bar()
  plt.xlabel(features)
  plt.ylabel('Saleprice')
  plt.show()

df.head()

X = df.drop(['SalePrice'], axis= 1)
y = df.loc[:, 'SalePrice']
X.head()

y.head()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test= train_test_split(X, y, test_size= 0.1, random_state= 0)

Nan_categor_features= [features for features in df.columns if df[features].isnull().sum()>1 and 
                       df[features].dtypes == 'O']
for features in Nan_categor_features:
  print('{}: {}% missing values'.format(features, np.round(df[features].isnull().mean(), 2)))

##
df[Nan_categor_features] = df[Nan_categor_features].fillna('Missing')
df[Nan_categor_features].isnull().sum()

df.head()

Nan_numer_features = [features for features in df.columns if df[features].isnull().sum()>1 and 
                      df[features].dtypes != 'O']
for features in Nan_numer_features:
  print('{} missing values {}%'.format(features, np.round(df[features].isnull().mean(), 2)))

##
for features in Nan_numer_features:
  median_val = df[features].median()
  df[features+'nan'] = np.where(df[features].isnull(), 1, 0)
  df[features].fillna(median_val, inplace= True)
df[Nan_numer_features].isnull().sum()

df.head(15)

##
for features in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:
  df[features]= df['YrSold'] - df[features]
df.head(10)

numer_features

continuous_features

##
continuous_numer_features= ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']
for features in continuous_numer_features:
  df[features] = np.log(df[features])

df.head(10)

df.shape

##
for features in categorical_features:
  temp= df.groupby(features)['SalePrice'].count()/len(df)
  temp_df= temp[temp>0.1].index
  df[features]= np.where(df[features].isin(temp_df),df[features], 'Rare_val')

df.head(50)

##
from sklearn.preprocessing import LabelEncoder
le= LabelEncoder()
for features in categorical_features:
  df[features]= le.fit_transform(df[features])

df.head(100)

feature_scaling= [features for features in df.columns if features not in ['Id', 'SalePrice']]
len(feature_scaling)

##
from sklearn.preprocessing import MinMaxScaler
scaler= MinMaxScaler()
scaler.fit(df[feature_scaling])

new_data= pd.concat([df[['Id', 'SalePrice']].reset_index(drop=True),
                    pd.DataFrame(scaler.transform(df[feature_scaling]), columns=feature_scaling)],
                    axis=1)

new_data.head(10)

df_test= pd.read_csv('test.csv')

df_test.head()

Nan_categor_features= [features for features in df_test.columns if df_test[features].isnull().sum()>1 and 
                       df_test[features].dtypes == 'O']
df_test[Nan_categor_features] = df[Nan_categor_features].fillna('Missing')
df_test[Nan_categor_features].isnull().sum()

Nan_numer_features = [features for features in df_test.columns if df_test[features].isnull().sum()>1 and 
                      df_test[features].dtypes != 'O']
for features in Nan_numer_features:
  median_val = df_test[features].median()
  df_test[features+'nan'] = np.where(df_test[features].isnull(), 1, 0)
  df_test[features].fillna(median_val, inplace= True)
df_test[Nan_numer_features].isnull().sum()

for features in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:
  df_test[features]= df_test['YrSold'] - df_test[features]
df_test.head(10)

##t
continuous_numer_features= ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']
for features in continuous_numer_features:
  df_test[features] = np.log(df_test[features])

categorical_features = [features for features in df_test.columns if df_test[features].dtypes == 'O']
print('No. of categorical features: ', len(categorical_features))

for features in categorical_features:
  temp= df_test.groupby(features).count()/len(df_test)
  temp_df= temp[temp>0.1].index
  df_test[features]= np.where(df_test[features].isin(temp_df),df_test[features], 'Rare_val')

from sklearn.preprocessing import LabelEncoder
le= LabelEncoder()
for features in categorical_features:
  df_test[features]= le.fit_transform(df_test[features])

feature_scaling= [features for features in df_test.columns if features not in ['Id']]
len(feature_scaling)

from sklearn.preprocessing import MinMaxScaler
scaler1= MinMaxScaler()
scaler1.fit(df_test[feature_scaling])

df_test.shape, df.shape

df.head()

df_test.head()

df_test.drop(['BsmtFullBathnan', 'BsmtHalfBathnan'], axis= 1, inplace= True)

df_test.shape, df.shape

y_train= df[['SalePrice']]

X_train= df.drop(['Id','SalePrice'], axis= 1)

X_train.head()

from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import Lasso
feature_sel_model = SelectFromModel(Lasso(alpha=0.005, random_state=0)) 
feature_sel_model.fit(X_train, y_train)

feature_sel_model.get_support()

selected_feat = X_train.columns[(feature_sel_model.get_support())]
print('total features: {}'.format((X_train.shape[1])))
print('selected features: {}'.format(len(selected_feat)))

selected_feat

X_train = X_train[selected_feat]

df_test= df_test[selected_feat]

X_train.head()

y_train.head()

n_estimators = [100, 500, 900, 1100, 1500]
max_depth = [2, 3, 5, 10, 15]
booster=['gbtree','gblinear']
learning_rate=[0.05,0.1,0.15,0.20]
min_child_weight=[1,2,3,4]
base_score=[0.25,0.5,0.75,1]

hyperparameter_grid = {
    'n_estimators': n_estimators,
    'max_depth':max_depth,
    'learning_rate':learning_rate,
    'min_child_weight':min_child_weight,
    'booster':booster,
    'base_score':base_score
    }

from xgboost import XGBRegressor
regressor = XGBRegressor()

from sklearn.model_selection import  RandomizedSearchCV
random_cv = RandomizedSearchCV(estimator=regressor,
            param_distributions=hyperparameter_grid,
            cv=5, n_iter=50,
            scoring = 'neg_mean_absolute_error',n_jobs = 4,
            verbose = 5, 
            return_train_score = True,
            random_state=42)

random_cv.fit(X_train, y_train)

random_cv.best_estimator_

regressor= XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0,
             importance_type='gain', learning_rate=0.05, max_delta_step=0,
             max_depth=2, min_child_weight=4, missing=None, n_estimators=900,
             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
             silent=None, subsample=1, verbosity=1)

regressor.fit(X_train, y_train)

y_pred = regressor.predict(df_test)

y_pred.reshape(len(y_pred), 1)

y_pred= np.exp(y_pred)

pred = pd.DataFrame(y_pred)
sub_df=pd.read_csv('sample_submission.csv')
datasets=pd.concat([sub_df['Id'],pred],axis=1)
datasets.columns=['Id','SalePrice']
datasets.to_csv('sample_submission.csv',index=False)

datasets.head()

